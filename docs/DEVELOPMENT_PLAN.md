# Chronos-X Development Plan: Completing the Specification

**Status**: Phase 1-2 Complete (60% Overall)
**Last Updated**: 2026-01-22

## Executive Summary

The Chronos-X trading system has a solid foundational implementation with the core CryptoMamba model, training infrastructure, and trading execution framework complete. This plan outlines the systematic completion of remaining components from the detailed technical specification.

---

## Current State Analysis

### ‚úÖ Implemented Features (80% Complete)

1. **Core ML Architecture**
   - CryptoMamba SSM model with 542K parameters
   - Dual-head architecture (classification + regression)
   - Auxiliary supervision heads
   - Ensemble support with uncertainty estimation

2. **Data Infrastructure** ‚úÖ ENHANCED
   - ClickHouse schema and client with connection pooling
   - Real-time WebSocket streaming (Binance)
   - L2 orderbook snapshots (depth20@100ms)
   - CCXT-based historical data loading
   - **NEW: Tardis.dev integration for L2/L3 tick data**
   - **NEW: CoinAPI multi-exchange support**
   - **NEW: L3 orderbook and nanosecond-precision trades schema**

3. **Feature Engineering** ‚úÖ ENHANCED
   - Fractional differentiation (FFD) with optimal d-finding
   - Triple Barrier labeling with volatility-based barriers
   - Basic OHLC features and moving averages
   - Volume imbalance calculations
   - **NEW: OFI and VPIN integrated into training pipeline**
   - **NEW: Effective spread and realized spread**
   - **NEW: Microstructure processor with batch support**

4. **Training & Validation**
   - Multi-task loss (Focal + Huber + auxiliary)
   - Advanced optimizers (AdamW, LR scheduling, AMP)
   - Combinatorial Purged Cross-Validation (CPCV)
   - MLflow integration for experiment tracking
   - Early stopping and checkpointing

5. **Meta-Labeling**
   - XGBoost/LightGBM secondary filter
   - Stacked ensemble support
   - Signal strength computation

6. **Production Infrastructure**
   - Async live trading system
   - Paper trading mode
   - HRP portfolio allocation
   - Volatility targeting position sizing
   - TWAP/VWAP execution algorithms
   - Drift detection and performance monitoring

7. **Sentiment & LLM Integration** ‚úÖ NEW
   - **Sentiment data collector (Twitter, Reddit, News)**
   - **Advanced sentiment analyzer (TextBlob, VADER, FinBERT, crypto lexicon)**
   - **LLM-based market regime detector**
   - **Fact vs subjectivity classification**
   - **Sentiment signals ClickHouse table with aggregations**

---

## Implementation Status by Phase

### ‚úÖ Phase 1: Enhanced Data Infrastructure (COMPLETED)

#### 1.1 Professional Data Source Integration ‚úÖ
- **Files Created:**
  - `chronos_x/data/ingestion/tardis_loader.py` - Tardis.dev integration
  - `chronos_x/data/ingestion/coinapi_client.py` - CoinAPI client
  - `config/data/tardis.yaml` - Tardis configuration
  - `config/data/coinapi.yaml` - CoinAPI configuration

**Features:**
- Historical L2/L3 orderbook reconstruction
- Tick-by-tick trade data with microsecond precision
- Event-driven orderbook updates
- Multi-exchange normalization
- Rate limiting and quota management
- Fallback to CCXT for free tier

#### 1.2 ClickHouse Schema Enhancements ‚úÖ
- **File Modified:** `data/schema/market_trades.sql`

**Added Tables:**
- `orderbook_l3` - Order-by-order changes with microsecond timestamps
- `trades_hf` - High-frequency trades with nanosecond precision
- `orderbook_snapshots_hf` - Orderbook snapshots with microsecond precision
- Additional indices for performance

#### 1.3 Microstructure Feature Integration ‚úÖ
- **File Modified:** `chronos_x/scripts/train_model.py`

**Changes:**
- Integrated `MicrostructureProcessor` into `prepare_features()`
- Added `--use-microstructure` flag to training commands
- Automatic computation of OFI, VPIN, spreads, volatility metrics
- Feature dimension expansion (64 ‚Üí 80+ features)

---

### ‚úÖ Phase 2: Sentiment & LLM Integration (PARTIALLY COMPLETED)

#### 2.1 Sentiment Data Sources ‚úÖ
- **Files Created:**
  - `chronos_x/data/ingestion/sentiment_collector.py`
  - `chronos_x/models/agents/sentiment_analyzer.py`
  - `config/data/sentiment_sources.yaml`

**Features:**
- Twitter/X sentiment collection (with API support)
- Reddit sentiment via PRAW
- News RSS feed aggregation
- On-chain metrics placeholders (Glassnode)
- Sentiment aggregation with engagement weighting
- ClickHouse sentiment_signals table with hourly aggregation views

#### 2.2 LLM-Based Regime Detection ‚úÖ
- **Files Created:**
  - `chronos_x/models/agents/regime_detector.py`

**Features:**
- LLM-powered regime classification (OpenAI GPT integration)
- Technical indicator-based regime detection
- Regime embeddings (128-dimensional vectors)
- Supported regimes:
  - High Volatility
  - Trending Up/Down
  - Sideways
  - Bearish Crash
  - Bull Market
  - Accumulation/Distribution

#### 2.3 Regime Embeddings for CryptoMamba ‚è≥ PENDING
- **File to Modify:** `chronos_x/models/mamba/crypto_mamba.py`

**Planned Changes:**
```python
def forward(self, x, regime_embedding=None):
    h = self.encoder(x)
    if regime_embedding is not None:
        h = h + self.regime_projection(regime_embedding)
    # ... rest of forward pass
```

#### 2.4 Sentiment Processor ‚è≥ PENDING
- **File to Create:** `chronos_x/data/processing/sentiment_processor.py`

**Planned Features:**
- Fact signal extraction (official announcements, regulatory filings)
- Subjectivity signal extraction (social media, influencer tweets)
- Dual-channel feature integration

---

### ‚è≥ Phase 3: Production Meta-Labeling Pipeline (PRIORITY: HIGH)

#### 3.1 Meta-Labeler Training Integration ‚è≥ PENDING
- **File to Modify:** `chronos_x/scripts/train_model.py`

**Planned Changes:**
```python
# After training CryptoMamba
primary_predictions = model.predict(X_val)
meta_labeler = MetaLabeler(config)
meta_labeler.fit(
    X=feature_matrix,
    y_primary=primary_predictions,
    y_actual=actual_labels
)
meta_labeler.save("models/meta_labeler.pkl")
```

**CLI Addition:**
```bash
python train_model.py train --use-meta-labeling
```

#### 3.2 Live Trading Integration ‚è≥ PENDING
- **Files to Modify:**
  - `chronos_x/production/live_trader.py`
  - `chronos_x/strategy/signal_generator.py`

**Planned Changes:**
```python
# In LiveTrader
self.meta_labeler = MetaLabeler.load(config.meta_labeler_path)

# In _generate_prediction()
primary_signal = self.model.predict(x_tensor)
meta_prob = self.meta_labeler.predict_proba(features)[0, 1]

if meta_prob < config.meta_threshold:  # e.g., 0.65
    return Signal(signal_type=SignalType.NEUTRAL)

position_size = base_size * meta_prob  # Confidence scaling
```

#### 3.3 Configuration ‚è≥ PENDING
**File to Create:** `config/meta_labeling.yaml`

```yaml
meta_labeling:
  enabled: true
  model_path: models/meta_labeler.pkl
  probability_threshold: 0.65
  confidence_scaling: true
  retrain_frequency_days: 7
```

---

### ‚è≥ Phase 4: Advanced Execution & Risk (PRIORITY: MEDIUM)

#### 4.1 Enhanced Execution Algorithms ‚è≥ PENDING
- **File to Modify:** `chronos_x/strategy/execution.py`

**New Algorithms:**
1. **Implementation Shortfall**
   - Minimize tracking error vs VWAP
   - Adaptive urgency based on alpha decay

2. **Participation Rate**
   - Target % of market volume
   - Passive-aggressive hybrid

3. **Smart Order Routing (SOR)**
   - Multi-venue execution
   - Venue selection by liquidity + latency

#### 4.2 Advanced Risk Management ‚è≥ PENDING
- **File to Modify:** `chronos_x/strategy/risk.py`

**Enhancements:**
1. **CDaR (Conditional Drawdown at Risk)**
   - Replace traditional VaR
   - Focus on tail risk of drawdown distribution

2. **Dynamic Position Scaling**
   - Scale by model confidence (meta-labeler)
   - Current drawdown level
   - Regime volatility

3. **Correlation-Based Limits**
   - Maximum correlation between open positions
   - Diversification enforcement

---

### ‚è≥ Phase 5: Hyperparameter Optimization (PRIORITY: LOW)

#### 5.1 Optuna Integration ‚è≥ PENDING
- **Files to Create:**
  - `chronos_x/scripts/optimize_hyperparameters.py`
  - `config/sweep/optimizing_dsr.yaml`

**Implementation:**
```python
def objective(trial):
    # Model hyperparameters
    hidden_dim = trial.suggest_int('hidden_dim', 128, 512, step=64)
    num_layers = trial.suggest_int('num_layers', 2, 6)

    # Labeling hyperparameters
    tp_multiple = trial.suggest_float('tp_multiple', 1.0, 3.0)
    sl_multiple = trial.suggest_float('sl_multiple', 1.0, 3.0)

    # Train model with params
    model = train_with_config(...)

    # Return DSR
    return compute_deflated_sharpe_ratio(...)
```

**Multi-Objective Optimization:**
- Objective 1: Maximize DSR
- Objective 2: Minimize Max Drawdown
- Objective 3: Maximize Calmar Ratio
- Use NSGAIIsampler for Pareto front

**CLI:**
```bash
python optimize_hyperparameters.py \
  --n-trials 100 \
  --study-name chronos-x-optimization \
  --storage sqlite:///optuna.db
```

---

## File Structure Summary

### Files Created ‚úÖ
```
chronos_x/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ ingestion/
‚îÇ       ‚îú‚îÄ‚îÄ tardis_loader.py          ‚úÖ NEW (500+ lines)
‚îÇ       ‚îú‚îÄ‚îÄ coinapi_client.py         ‚úÖ NEW (450+ lines)
‚îÇ       ‚îî‚îÄ‚îÄ sentiment_collector.py    ‚úÖ NEW (550+ lines)
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ agents/
‚îÇ       ‚îú‚îÄ‚îÄ sentiment_analyzer.py     ‚úÖ NEW (600+ lines)
‚îÇ       ‚îî‚îÄ‚îÄ regime_detector.py        ‚úÖ NEW (650+ lines)

config/
‚îî‚îÄ‚îÄ data/
    ‚îú‚îÄ‚îÄ tardis.yaml                   ‚úÖ NEW
    ‚îú‚îÄ‚îÄ coinapi.yaml                  ‚úÖ NEW
    ‚îî‚îÄ‚îÄ sentiment_sources.yaml        ‚úÖ NEW
```

### Files Modified ‚úÖ
```
chronos_x/
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ train_model.py                ‚úÖ MODIFIED (added microstructure support)

data/
‚îî‚îÄ‚îÄ schema/
    ‚îî‚îÄ‚îÄ market_trades.sql             ‚úÖ MODIFIED (added L3, HF, sentiment tables)
```

### Files Pending ‚è≥
```
chronos_x/
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ mamba/
‚îÇ       ‚îî‚îÄ‚îÄ crypto_mamba.py           ‚è≥ MODIFY (regime embedding support)
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ processing/
‚îÇ       ‚îî‚îÄ‚îÄ sentiment_processor.py    ‚è≥ CREATE
‚îú‚îÄ‚îÄ production/
‚îÇ   ‚îî‚îÄ‚îÄ live_trader.py                ‚è≥ MODIFY (meta-labeling)
‚îú‚îÄ‚îÄ strategy/
‚îÇ   ‚îú‚îÄ‚îÄ signal_generator.py           ‚è≥ MODIFY (confidence filtering)
‚îÇ   ‚îú‚îÄ‚îÄ execution.py                  ‚è≥ MODIFY (new algorithms)
‚îÇ   ‚îî‚îÄ‚îÄ risk.py                       ‚è≥ MODIFY (CDaR, dynamic scaling)
‚îî‚îÄ‚îÄ scripts/
    ‚îî‚îÄ‚îÄ optimize_hyperparameters.py   ‚è≥ CREATE

config/
‚îú‚îÄ‚îÄ meta_labeling.yaml                ‚è≥ CREATE
‚îî‚îÄ‚îÄ sweep/
    ‚îî‚îÄ‚îÄ optimizing_dsr.yaml           ‚è≥ CREATE
```

---

## Verification & Testing

### Phase 1 Verification ‚úÖ
1. **Data Quality:**
   - ‚úÖ Tardis and CoinAPI modules created with async support
   - ‚úÖ ClickHouse schema includes L3, HF, and microstructure tables
   - ‚úÖ Configuration files with environment variable support

2. **Feature Validation:**
   - ‚úÖ Microstructure features integrated into training pipeline
   - ‚úÖ `--use-microstructure` flag added to train/cross-validate commands
   - ‚è≥ Pending: Actual testing with market data

### Phase 2 Verification ‚úÖ / ‚è≥
1. **Sentiment Integration:**
   - ‚úÖ Sentiment collector supports Twitter, Reddit, News
   - ‚úÖ Advanced analyzer with multiple models
   - ‚úÖ ClickHouse tables for sentiment storage
   - ‚è≥ Pending: Reddit API credentials testing
   - ‚è≥ Pending: Sentiment-price correlation analysis

2. **Regime Detection:**
   - ‚úÖ Indicator-based regime detection
   - ‚úÖ LLM regime detection with OpenAI
   - ‚úÖ Regime embedding generation (128-dim)
   - ‚è≥ Pending: CryptoMamba conditioning implementation
   - ‚è≥ Pending: Backtest with/without regime embeddings

### Phase 3 Verification ‚è≥
1. **Meta-Labeling Performance:**
   - ‚è≥ Pending: Precision/recall improvement vs primary model
   - ‚è≥ Pending: Win rate increase with filtering
   - ‚è≥ Pending: False positive reduction

2. **Live Trading:**
   - ‚è≥ Pending: Paper trading for 1 week with meta-labeling
   - ‚è≥ Pending: Filtered vs unfiltered signal comparison
   - ‚è≥ Pending: Execution quality monitoring

---

## Dependencies & API Keys

### Required
- ‚úÖ Python 3.10+
- ‚úÖ ClickHouse
- ‚úÖ CCXT (free tier)

### Optional (Enhanced Features)
- **Tardis.dev** ($200-500/month) - HF tick data
  - Set: `TARDIS_API_KEY`
- **CoinAPI** ($500-1000/month) - Multi-exchange data
  - Set: `COINAPI_KEY` (default provided: 986e7da4-750a-4645-a3d4-346cd965b8c0)
- **Twitter API v2** ($100/month) - Sentiment
  - Set: `TWITTER_API_KEY`, `TWITTER_API_SECRET`, `TWITTER_BEARER_TOKEN`
- **Reddit API** (free) - Sentiment
  - Set: `REDDIT_CLIENT_ID`, `REDDIT_CLIENT_SECRET`
- **Glassnode** ($400/month) - On-chain metrics
  - Set: `GLASSNODE_API_KEY`
- **OpenAI API** (~$20-50/month) - LLM regime detection
  - Set: `OPENAI_API_KEY`
- **Hugging Face** (free) - FinBERT sentiment
  - Automatic download on first use

---

## Success Metrics

### Phase 1-2 Targets ‚úÖ
- ‚úÖ All data infrastructure modules created
- ‚úÖ Microstructure features integrated
- ‚úÖ Sentiment collection and analysis implemented
- ‚úÖ Schema enhancements complete

### Phase 3 Targets ‚è≥
- DSR > 0.95 (95% confidence non-random)
- Sharpe > 1.5 in live trading
- Win rate > 55% with meta-labeling
- False positive reduction > 20%

### Phase 4 Targets ‚è≥
- Average slippage < 5 bps
- 99th percentile latency < 50ms tick-to-trade
- Max drawdown < 15%
- Calmar ratio > 2.0

### Phase 5 Targets ‚è≥
- 100+ Optuna trials completed
- Pareto front with 10+ non-dominated solutions
- Out-of-sample validation DSR improvement > 10%

---

## Timeline Estimates

- **Phase 1 (Data Infrastructure):** ‚úÖ COMPLETED (3 days)
- **Phase 2 (Sentiment/LLM):** üü® 80% COMPLETE (2 days remaining)
- **Phase 3 (Meta-Labeling):** ‚è≥ PENDING (1-2 weeks)
- **Phase 4 (Execution/Risk):** ‚è≥ PENDING (2-3 weeks)
- **Phase 5 (Optimization):** ‚è≥ PENDING (1-2 weeks)

**Total Remaining:** 5-8 weeks for complete implementation

---

## Next Steps

### Immediate (This Week)
1. ‚è≥ Implement regime embedding support in CryptoMamba
2. ‚è≥ Create sentiment processor for fact/subjectivity signals
3. ‚è≥ Test sentiment collection with live data
4. ‚è≥ Integrate meta-labeling into training script

### Short-term (Next 2 Weeks)
1. ‚è≥ Deploy meta-labeling to live trading
2. ‚è≥ Add meta-labeling configuration
3. ‚è≥ Run paper trading with meta-labeling for 1 week
4. ‚è≥ Begin advanced execution algorithm implementation

### Medium-term (Next 4-6 Weeks)
1. ‚è≥ Complete execution algorithm suite
2. ‚è≥ Implement CDaR and advanced risk management
3. ‚è≥ Set up Optuna hyperparameter optimization
4. ‚è≥ Run comprehensive backtests

### Long-term (Next 2-3 Months)
1. ‚è≥ Production deployment with all features
2. ‚è≥ Live trading monitoring and refinement
3. ‚è≥ Performance analysis and optimization
4. ‚è≥ Documentation and maintenance

---

## Risk Mitigation

1. **Data Source Fallback:**
   - ‚úÖ CCXT maintained as free backup
   - ‚úÖ Graceful degradation implemented
   - ‚úÖ Configuration flags for optional sources

2. **LLM Rate Limiting:**
   - ‚úÖ Regime detection caching (1-hour TTL)
   - ‚úÖ Fallback to indicator-based detection
   - üü® TODO: Use smaller models for production

3. **Feature Dimension Management:**
   - ‚úÖ Dynamic feature selection in training
   - ‚è≥ TODO: SHAP/permutation importance analysis
   - ‚è≥ TODO: Feature selection optimization

4. **Backward Compatibility:**
   - ‚úÖ All new features have enable/disable flags
   - ‚úÖ Training works without microstructure features
   - ‚úÖ System operational with CCXT only

---

## Appendix: Key Code Snippets

### Training with Microstructure Features
```bash
# Enable microstructure features (OFI, VPIN, etc.)
python chronos_x/scripts/train_model.py train \
  --symbol BTC/USDT \
  --use-microstructure \
  --epochs 100

# Disable for baseline comparison
python chronos_x/scripts/train_model.py train \
  --symbol BTC/USDT \
  --use-microstructure false
```

### Sentiment Collection
```python
from chronos_x.data.ingestion.sentiment_collector import create_sentiment_collector

collector = create_sentiment_collector(
    reddit_enabled=True,
    reddit_client_id="your_id",
    reddit_client_secret="your_secret"
)

sentiment_data = await collector.collect_all()
btc_sentiment = collector.aggregate_sentiment(sentiment_data, "BTC")
```

### Regime Detection
```python
from chronos_x.models.agents.regime_detector import create_regime_detector

detector = create_regime_detector(use_llm=True)
result = detector.detect(prices, returns, sentiment=0.3)

print(f"Regime: {result.regime.value}")
print(f"Confidence: {result.confidence:.2f}")
print(f"Embedding: {result.regime_embedding.shape}")
```

---

## Change Log

### 2026-01-22 - Phase 1-2 Implementation
- ‚úÖ Created 8 new modules (3,000+ lines of code)
- ‚úÖ Enhanced 2 existing modules
- ‚úÖ Added 3 configuration files
- ‚úÖ Extended ClickHouse schema with 5 new tables
- ‚úÖ Integrated microstructure features into training
- ‚úÖ Implemented sentiment analysis and regime detection
- üéØ 60% overall completion (up from 40%)

### Next Update Target
- Week of 2026-01-29: Phase 2.3 and Phase 3 completion

---

**Document Version:** 2.0
**Generated By:** Claude Sonnet 4.5
**Project:** Chronos-X Trading System
**Repository:** https://github.com/mwridgway/chronos_x
